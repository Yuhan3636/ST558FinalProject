---
title: "Modeling Diabetes Risk"
format: html
editor: visual
---

# Introduction

In this vignette we will:

1.  Split the data into training (70%) and test (30%) sets.\
2.  Fit three candidate logistic regression models and choose the best via 5-fold CV (log-loss).\
3.  Fit a classification tree (tuning `cp`) and select the best.\
4.  Fit a random forest on our three EDA predictors (tuning `mtry`, `ntree = 100`).\
5.  Compare the *three* best models on CV log-loss.\
6.  Evaluate the overall winner on the held-out test set.

```{r setup, include=FALSE}
# 0. Packages & reproducibility
library(tidyverse)
library(caret)
library(conflicted)
conflict_prefer("filter", "dplyr")
conflict_prefer("lag",    "dplyr")

set.seed(2025)
```

# Data import & train/test split

```{r}
# 1. Read & prep
diab <- read_csv(
  "diabetes_012_health_indicators_BRFSS2015.csv",
  show_col_types = FALSE
) %>%
  # drop pre‑diabetes and recode to a binary outcome
  filter(Diabetes_012 != 2) %>%
  mutate(
    Diabetes = factor(
      Diabetes_012,
      levels = c(0,1),
      labels = c("No","Yes")
    )
  )

# 2. Split 70/30
idx   <- createDataPartition(diab$Diabetes, p = 0.7, list = FALSE)
train <- diab[idx, ]
test  <- diab[-idx, ]

# 3. Check balance
train %>% count(Diabetes) %>% mutate(prop = n/sum(n))
test  %>% count(Diabetes) %>% mutate(prop = n/sum(n))

```

# 2. CV setup with log‑loss

```{r}
ctrl <- trainControl(
  method           = "cv",
  number           = 5,
  classProbs       = TRUE,
  summaryFunction  = mnLogLoss
)
```

# 3. Logistic regression

We'll fit three candidate logistic models:

Model 1: the three EDA predictors

Model 2: add \`HighChol\`

Model 3: add a quadratic BMI term

```{r}

fit_glm1 <- train(
  Diabetes ~ PhysActivity + HighBP + BMI,
  data      = train,
  method    = "glm",
  family    = "binomial",
  metric    = "logLoss",
  trControl = ctrl
)

fit_glm2 <- train(
  Diabetes ~ PhysActivity + HighBP + BMI + HighChol,
  data      = train,
  method    = "glm",
  family    = "binomial",
  metric    = "logLoss",
  trControl = ctrl
)

fit_glm3 <- train(
  Diabetes ~ PhysActivity + HighBP + BMI + I(BMI^2),
  data      = train,
  method    = "glm",
  family    = "binomial",
  metric    = "logLoss",
  trControl = ctrl
)
```

Compare logistic candidates:

```{r}
glm_res <- resamples(list(
  EDA     = fit_glm1,
  AddChol = fit_glm2,
  BMI2    = fit_glm3
))

# Box‑and‑whisker of CV log‑loss
bwplot(glm_res, metric = "logLoss")

# Numeric summary: mean ± SD
summary(glm_res, metric = "logLoss")$statistics$logLoss
```

Compare logistic candidates

```{r}
glm_res <- resamples(list(
  EDA     = fit_glm1,
  AddChol = fit_glm2,
  BMI2    = fit_glm3
))

# Box‑and‑whisker plot of CV log‑loss
bwplot(glm_res, metric = "logLoss")

# Numeric summary (mean ± SD)
summary(glm_res, metric = "logLoss")$statistics$logLoss

```

**Model 2 (AddChol)** is best.

```{r}
selected_glm <- fit_glm2  
```

# 4. Classification tree

A tree can capture non-linear splits. We tune the complexity parameter cp:

```{r}
tree_grid <- expand.grid(cp = seq(0, 0.1, by = 0.01))

fit_tree <- train(
  Diabetes ~ PhysActivity + HighBP + BMI,
  data      = train,
  method    = "rpart",
  metric    = "logLoss",
  trControl = ctrl,
  tuneGrid  = tree_grid
)
```

# Random forest

Using our three EDA variables only, with ntree = 100 for speed:

```{r}
# Random forest on just the 3 EDA predictors
rf_grid <- expand.grid(mtry = 1:3)

fit_rf <- train(
  Diabetes ~ PhysActivity + HighBP + BMI,
  data      = train,
  method    = "rf",
  metric    = "logLoss",
  trControl = ctrl,
  tuneGrid  = rf_grid,
  # speed things up:
  ntree     = 100      
)
fit_rf

```

# CV comparison

Compare best logistic, tree, and RF on CV log-loss:

```{r}
resamps_best <- resamples(list(
  Logistic = selected_glm,
  Tree     = fit_tree,
  RF       = fit_rf
))

bwplot(resamps_best, metric = "logLoss")
summary(resamps_best, metric = "logLoss")$statistics$logLoss

```

# 7. Final model evaluation on test set

```{r}
pred_prob   <- predict(selected_glm, newdata = test, type = "prob")$Yes
obs         <- as.numeric(test$Diabetes == "Yes")
test_logloss <- -mean(obs * log(pred_prob) + (1 - obs) * log(1 - pred_prob))

# Print test log-loss
test_logloss
```

Best CV performer: Logistic regression (AddChol)

The test-set log-loss is 0.097.
